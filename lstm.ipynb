{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aZnF-8Pked6P"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from numpy import array\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ISbI1UwGev90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 19:11:33.703650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-15 19:11:33.703674: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# required for training only\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Dropout\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tBAImUW_ev0j"
   },
   "outputs": [],
   "source": [
    "#@title TRAINING CLASS WITH TRY EXCEPT\n",
    "\n",
    "class LSTM_MODEL:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------------------------------------------------------------------------\n",
    "    df : DataFrame\n",
    "        Input DataFrame \n",
    "\n",
    "    model_1_independent_features : list\n",
    "        List of the independent features to train model 1.\n",
    "\n",
    "    model_1_target_feature : str\n",
    "        Target feature name to train model 1.\n",
    "\n",
    "    n_steps : tuple \n",
    "        Tuple of integer values from which model will find best n_step value.\n",
    "        e.g (1,10) -> model will find best_n_step from this range\n",
    "\n",
    "    train_size : float\n",
    "        float value which indicates the percentage of fraction for train split.\n",
    "        e.g 0.70 -> in will take 70% of data for training\n",
    "\n",
    "    root_path : str\n",
    "        Root path where model will be stored.\n",
    "        \n",
    "    Methods\n",
    "    ----------------------------------------------------------------------------\n",
    "    get_records_with_akm_combinations(df, AD_GROUP_ID, KEYWORD_ID, MATCH_TYPE)\n",
    "        -> Return the pandas dataframe with given AKM combination\n",
    "\n",
    "    data_preprocessing(df,features)\n",
    "        -> It remove rows with all zeroes or Nan values for given subset of features\n",
    "\n",
    "    train_test_split(dataset,train_size)\n",
    "        -> Split the dataset into train and test data based on train_size fraction value.\n",
    "\n",
    "    split_sequences(sequences, n_steps)\n",
    "        -> It returns independent feature value sequences along with target feature value\n",
    "           based on n_steps value to feed into LSTM model\n",
    "        \n",
    "    split_sequences_inference(sequences, n_steps)\n",
    "        -> It returns test sequences based on n_steps to get prediction from LSTM model.\n",
    "\n",
    "    serialize_model(self,model,root_path,AD_GROUP_ID,KEYWORD_ID,MATCH_TYPE,target_feature)\n",
    "        -> It will serialize model and dump it at given root_path location with AKM values with target_feature as model name in \".h5\" format\n",
    "\n",
    "    get_stats(self,df,feature)\n",
    "        -> It returns min, max, mean, std and max_roc for the given feature from the training data\n",
    "\n",
    "    smape(self,actual, predicted)\n",
    "        -> It return symetric mean absolute percentage error\n",
    "\n",
    "    rolling_diff(self,df,target_feature,n_step)\n",
    "        -> It calculates delta value from the current predicted value and previous real value for the given feature\n",
    "\n",
    "    serialize_akm_metadata_model_1(self,root_path=None,df = None,metric_df = None,independent_features = None,inference_date = None,onnx_output_names=None)\n",
    "        -> It will serialize AKM values along with the stats of model 1 as a json and store at a given location.\n",
    "\n",
    "    append_pred_result(df,y_pred,n_steps,target_feature)\n",
    "        -> It will return dataframe with predicted target value appended to the original dataframe based on n_step value\n",
    "\n",
    "    get_metrics_df(df,n_steps = (1,20),train_size=0.70)\n",
    "        -> It will return dataframe with diffrent n_step_values with corresponding metrics (RMSE, R2) of train and test data.\n",
    "\n",
    "    tuned_lstm(df,n_steps, train_size)\n",
    "        -> It will train LSTM model with given best n_step value and return dict of trained model object and metrics info.\n",
    "\n",
    "    min_max_scaling(self,train_data, test_data)\n",
    "        -> It will return scaled_train_data and scaled_test_data and scaler object as a dictionary.\n",
    "    \n",
    "    keras_to_onnx_model(self,keras_model,best_n_steps,n_features,root_path,AD_GROUP_ID,KEYWORD_ID,MATCH_TYPE,target_feature)\n",
    "        -> It will convert and store keras model as onnx and return output names of onnx model\n",
    "\n",
    "    lstm_model(self,train_data,test_data,n_steps)\n",
    "        -> It will return dictionary with trained model object and metrics values of train and test data.\n",
    "\n",
    "    tsa_model(self,df,independent_features,target_feature,n_steps,train_size,root_path,serialize_model=None)\n",
    "        -> It will train lstm model by finding best n_step value from the given range of n_steps and\n",
    "           serialize model and dump at given location if serialize_model = True.\n",
    "\n",
    "    model_1()\n",
    "        -> It will use tsa_model() method to train model by using model_1 independent_feature and target feature \n",
    "           and return dataframe with prediction appended to it, y_test and y_test_pred dataframe and metrics df.\n",
    "\n",
    "    main(self)\n",
    "        -> It will call model_1() method and return model_1() outputs\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 model_1_independent_features,\n",
    "                 model_1_target_feature,\n",
    "                 n_steps,\n",
    "                 train_size,\n",
    "                 root_path):\n",
    "        \n",
    "        self.df = df                                                     # input dataframe\n",
    "        self.model_1_independent_features = model_1_independent_features # model 1 independent features\n",
    "        self.model_1_target_feature = model_1_target_feature             # model 1 target features\n",
    "        self.n_steps = n_steps                                           # max n steps to be tuned e.g (1,10)\n",
    "        self.train_size = train_size                                     # train size e.g 0.70 (70%)\n",
    "        self.root_path = root_path                                       # root path when model would be stored\n",
    "        self.todays_date = datetime.now()\n",
    "\n",
    "  \n",
    "    def get_records_with_akm_combinations(self,df, AD_GROUP_ID, KEYWORD_ID, MATCH_TYPE):\n",
    "        '''\n",
    "        Args :\n",
    "            df -> Input dataframe\n",
    "            AD_GROUP_ID -> value of AD_GROUP_ID\n",
    "            KEYWORD_ID  -> value of KEYWORD_ID\n",
    "            MATCH_TYPE  -> value of MATCH_TYPE\n",
    "        Returns:\n",
    "            Returns filtered dataframe with given AKM combination\n",
    "        '''\n",
    "        df = df.loc[(df['AD_GROUP_ID'] == AD_GROUP_ID) & (df['KEYWORD_ID'] == KEYWORD_ID) & \n",
    "                    (df['MATCH_TYPE'] == MATCH_TYPE)].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def data_preprocessing(self,df,features):\n",
    "        '''\n",
    "        Args :\n",
    "            df -> Input dataframe\n",
    "            features -> features list to be preprocessed\n",
    "        Returns:\n",
    "            Remove rows with all 0s and NaN in a Dataframe'''\n",
    "        df = df.loc[(df[features] != 0).any(axis=1)].dropna(subset=features).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def train_test_split(self,dataset,train_size = 0.70):\n",
    "        '''\n",
    "        Args :\n",
    "            dataset -> full input dataframe\n",
    "            train_size -> train size in float\n",
    "        Returns :\n",
    "            Split into train and test sets and return train and test dataframes'''\n",
    "        train_size = int(len(dataset) * train_size)\n",
    "        test_size = len(dataset) - train_size\n",
    "        train, test = dataset.iloc[0:train_size,:], dataset.iloc[train_size:len(dataset),:]\n",
    "        return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "    def split_sequences(self,dataset,n_steps):\n",
    "        '''\n",
    "        Args :\n",
    "            dataset : input dataframe\n",
    "            n_steps : look back window size (int)   \n",
    "        Returns :\n",
    "            Split a multivariate sequence into samples to feed data in LSTM model\n",
    "        '''\n",
    "        # if n_steps is greater then or equal to length of data\n",
    "        if n_steps >= len(dataset):\n",
    "            n_steps = len(dataset) - 1\n",
    "\n",
    "        sequences = []\n",
    "        labels = []\n",
    "\n",
    "        start_idx = 0\n",
    "\n",
    "        for stop_idx in range(n_steps,len(dataset)):\n",
    "            sequences.append(dataset.iloc[start_idx:stop_idx,:-1])\n",
    "            labels.append(dataset.iloc[stop_idx,-1])\n",
    "            start_idx += 1\n",
    "\n",
    "        return (np.array(sequences),np.array(labels))\n",
    "\n",
    "    \n",
    "    # Function to create sequence of data for inference\n",
    "    def split_sequences_inference(self,sequences, n_steps):\n",
    "        '''\n",
    "        Args :\n",
    "            sequences -> input dataset as a series\n",
    "            n_steps -> look back window size(int) which was used during training\n",
    "        Returns : \n",
    "            It returns test sequences based on n_steps to get prediction from LSTM model.\n",
    "        '''\n",
    "        # if n_steps is greater then or equal to length of data\n",
    "        if n_steps >= len(sequences):\n",
    "            n_steps = len(sequences) - 1\n",
    "            \n",
    "        X = list()\n",
    "        for i in range(len(sequences)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + n_steps\n",
    "            # check if we are beyond the dataset\n",
    "            if end_ix > len(sequences):\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x = sequences[i:end_ix, :]\n",
    "            X.append(seq_x)\n",
    "        return array(X)\n",
    "            \n",
    "    def get_stats(self,df,feature):\n",
    "        '''\n",
    "        Args :\n",
    "            df -> input dataframe\n",
    "            feature -> feature name for which statistics to be returned\n",
    "        Returns :\n",
    "            It returns min, max, mean, std and max_roc for the given feature from the training data\n",
    "        '''\n",
    "        max_val = round(df[feature].max(),2)   # get maximum value for the given feature\n",
    "        mean_val = round(df[feature].mean(),2) # get mean value for the given feature\n",
    "        std_val = round(df[feature].std(),2)   # get standard deviation for the given feature\n",
    "        max_roc = round(df[f'{feature}_DELTA(%)'].max(),2)  # get maximum roc from the delta values\n",
    "\n",
    "        non_zero_indicies = df[feature].to_numpy().nonzero()  # find non zero value indicies for given feature\n",
    "        non_zero_values = df[feature].iloc[non_zero_indicies] # get the non zero value array\n",
    "        min_val = min(non_zero_values)                        # get the minimum value from the non zero values\n",
    " \n",
    "        return {'min':min_val,\n",
    "                'max':max_val,\n",
    "                'mean':mean_val,\n",
    "                'std':std_val,\n",
    "                'max_roc':max_roc}\n",
    "\n",
    "    \n",
    "    def smape(self,act,pred):\n",
    "        '''\n",
    "        Args :\n",
    "            actual -> array/series/list of actual values \n",
    "            predicted -> array/series/list of predicted values\n",
    "        Returns :\n",
    "            It return symetric mean absolute percentage error\n",
    "\n",
    "        Assumption: actuals and predictions are not logarithms\n",
    "        '''\n",
    "        \n",
    "        smape = 0\n",
    "        skippedcnt = 0\n",
    "        for a,p in zip(act,pred):\n",
    "            den = abs(a) + abs(p)\n",
    "            if (den > 0):\n",
    "                num = 2*abs(p-a)\n",
    "            else:\n",
    "                den = 1\n",
    "                num = 2*abs(p-a)\n",
    "                skippedcnt += 1\n",
    "                \n",
    "            smape += (num/den)\n",
    "        \n",
    "        smape = (100*smape)/len(act)\n",
    "        smapedatalength = len(act)\n",
    "                            \n",
    "        return smapedatalength, skippedcnt, round(smape,2)\n",
    "\n",
    "\n",
    "    def rolling_diff(self,df,target_feature,n_step):\n",
    "        '''\n",
    "        Args :\n",
    "            df -> input dataframe\n",
    "            target_feature -> Name of the target feature\n",
    "            n_step -> look back window size\n",
    "        Returns :\n",
    "            It calculates delta value from the current predicted value and previous real value for the given feature.\n",
    "        '''\n",
    "        i = 0 \n",
    "        diff_lst = []\n",
    "        for idx in range(len(df)):\n",
    "            if i < len(df) - n_step:\n",
    "                j = i + n_step\n",
    "                current_pred_val =  df[f'predicted_{target_feature}'].iloc[j] # get the current predicted value of the target fetaure\n",
    "                previous_real_val = df[target_feature].iloc[j-1]              # get the previous real value of the target feature\n",
    "                if previous_real_val != 0 :\n",
    "                    delta = round(((current_pred_val - previous_real_val)/previous_real_val)*100,2)  # calculate delta if previous real value is nonzero\n",
    "                else:\n",
    "                    # get previous values from the current index\n",
    "                    previous_values = df[target_feature].iloc[:idx]\n",
    "                    # get previous non zero indices\n",
    "                    previous_non_zero_indicies = previous_values.to_numpy().nonzero()[0]\n",
    "                    \n",
    "                    # check if there are any nonzero indicies or not\n",
    "                    if previous_non_zero_indicies.size > 0:\n",
    "                        last_non_zero_index = previous_non_zero_indicies[-1] # most recent non zero index\n",
    "                        previous_real_val = previous_values.iloc[last_non_zero_index] # most recent non zero value\n",
    "                    \n",
    "                    else:\n",
    "                        # find non zero value indicies for given feature\n",
    "                        non_zero_indicies = df[target_feature].to_numpy().nonzero()\n",
    "                        # get the non zero value array\n",
    "                        non_zero_values = df[target_feature].iloc[non_zero_indicies]\n",
    "                        # take the min value from the non zero values\n",
    "                        min_val = min(non_zero_values)\n",
    "                        previous_real_val = min_val\n",
    "                \n",
    "                    delta = round(((current_pred_val - previous_real_val)/previous_real_val)*100,2)\n",
    "\n",
    "                diff_lst.append(delta)\n",
    "                i = i+1\n",
    "        df.loc[n_step:, f'{target_feature}_DELTA(%)'] = diff_lst\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def serialize_akm_metadata_model_1(self,\n",
    "                           root_path=None, \n",
    "                           df = None,\n",
    "                           metric_df = None,\n",
    "                           independent_features = None,\n",
    "                           inference_date = None,\n",
    "                           onnx_output_names=None):\n",
    "        '''\n",
    "        Args :\n",
    "            root_path -> path where akm metadata of all the models to be stored.\n",
    "            df -> input dataframe \n",
    "            metric_df -> metric df with metric info of model 1\n",
    "            independent_features -> Independent feature list for model 1\n",
    "            inference_date -> t+1 date from the training data\n",
    "        Returns :\n",
    "            It will serialize AKM values along with the stats of model 1 as a json and store at a given location.\n",
    "        '''\n",
    "        # fetch value of AKM from the given df\n",
    "        AD_GROUP_ID = df.iloc[0]['AD_GROUP_ID']\n",
    "        KEYWORD_ID = df.iloc[0]['KEYWORD_ID']\n",
    "        MATCH_TYPE = df.iloc[0]['MATCH_TYPE']\n",
    "\n",
    "        # fetch the best n_step value for model 1\n",
    "        N_STEPS_MODEL_1 = int(metric_df.iloc[0]['BEST_N_STEPS'])\n",
    "\n",
    "        # fetch the MAPE and SMAPE value for model 1\n",
    "        model_1_test_mape = round(float(metric_df.iloc[0]['TEST_MAPE']), 2)\n",
    "        model_1_test_smape = round(float(metric_df.iloc[0]['TEST_SMAPE']), 2)\n",
    "\n",
    "        #get scaler from metric df\n",
    "        model_1_scaler = metric_df.iloc[0]['SCALER']\n",
    "        \n",
    "        # get model 1 statistics\n",
    "        model_1_stats = self.get_stats(df,self.model_1_target_feature)\n",
    "       \n",
    "        \n",
    "        # year\n",
    "        year = self.todays_date.strftime(\"%Y\")\n",
    "        # Month\n",
    "        month = self.todays_date.strftime(\"%m\")\n",
    "        # Day\n",
    "        day = self.todays_date.strftime(\"%d\")\n",
    "\n",
    "        year_month_day =  str(year) + str(month) + str(day)\n",
    "\n",
    "        model_1_name = '_'.join([year_month_day, \n",
    "                                 str(int(AD_GROUP_ID)),\n",
    "                                 str(int(KEYWORD_ID)),\n",
    "                                 str(MATCH_TYPE),\n",
    "                                 f'{self.model_1_target_feature}',\n",
    "                                 'LSTMModel'])\n",
    "\n",
    "        # model_1_name = '_'.join([year_month_day, \n",
    "        #                        str(int(AD_GROUP_ID)),\n",
    "        #                        str(int(KEYWORD_ID)),\n",
    "        #                        str(MATCH_TYPE),\n",
    "        #                        f'{self.model_1_target_feature}Model'])\n",
    "        \n",
    "        # create akm json to store at specified location\n",
    "        akm_dict = {'AD_GROUP_ID' : int(AD_GROUP_ID),\n",
    "                    'KEYWORD_ID' : int(KEYWORD_ID),\n",
    "                    'MATCH_TYPE' : str(MATCH_TYPE),\n",
    "                    'INFERENCE_DATE' : str(inference_date),\n",
    "                    'MODEL_VERSION' : str(self.todays_date),\n",
    "                    'MODEL_1' : {\n",
    "                        'NAME' : model_1_name,\n",
    "                        'INDEPENDENT_FEATURES' : independent_features,\n",
    "                        'TARGET_FEATURE' : self.model_1_target_feature,\n",
    "                        'ONNX_OUTPUT_NAMES' : onnx_output_names,\n",
    "                        'N_STEP' : N_STEPS_MODEL_1,\n",
    "                        'SCALER' : model_1_scaler,\n",
    "                        'STATS' : {\n",
    "                            f'{self.model_1_target_feature}_MIN' : model_1_stats['min'],\n",
    "                            f'{self.model_1_target_feature}_MAX' : model_1_stats['max'],\n",
    "                            f'{self.model_1_target_feature}_MEAN' : model_1_stats['mean'],\n",
    "                            f'{self.model_1_target_feature}_SD' : model_1_stats['std'],\n",
    "                            f'{self.model_1_target_feature}_MAX_ROC' : model_1_stats['max_roc']\n",
    "                        },\n",
    "                        'MODEL_STATS' : {\n",
    "                            'MAPE' : model_1_test_mape,\n",
    "                            'SMAPE' : model_1_test_smape\n",
    "                        }   \n",
    "                    }\n",
    "                }\n",
    "                \n",
    "        #metadata_file_name = '_'.join([str(int(AD_GROUP_ID)),str(int(KEYWORD_ID)),str(MATCH_TYPE)])\n",
    "        full_path = os.path.join(root_path,model_1_name)\n",
    "        # dump json at specified location\n",
    "        joblib.dump(akm_dict,filename=full_path+'.json')\n",
    "        print('AKM metadata Dictionary saved at {}'.format(full_path))\n",
    "        return 'AKM metadata Dictionary saved at {}'.format(full_path)\n",
    "\n",
    "    \n",
    "    def append_pred_result(self,df,y_pred,n_steps,target_feature):\n",
    "        '''\n",
    "        Args :\n",
    "            df -> input dataframe to which predicted column will be appended\n",
    "            y_pred -> predicted series/list of the given target feature\n",
    "            n_steps -> look back window size\n",
    "            target_feature -> name of the target feature\n",
    "        Returns :\n",
    "            It will return dataframe with predicted target value appended to the original dataframe based on n_step value\n",
    "        '''\n",
    "        # df : dataframe of the specific AKM combination\n",
    "        df_out = df.copy()\n",
    "        AD_GROUP_ID = int(df_out.iloc[0]['AD_GROUP_ID'])\n",
    "        KEYWORD_ID = int(df_out.iloc[0]['KEYWORD_ID'])\n",
    "        MATCH_TYPE = str(df_out.iloc[0]['MATCH_TYPE'])\n",
    "        \n",
    "        df_out['REPORT_DATE'] = pd.to_datetime(df_out['REPORT_DATE'],yearfirst = True)\n",
    "        # assign predicted target feature to the dataframe\n",
    "        df_out.loc[n_steps:,f'predicted_{target_feature}'] = y_pred[:-1]\n",
    "        # get next date\n",
    "        next_date = df_out.iloc[-1]['REPORT_DATE'] + pd.Timedelta(days=1)\n",
    "        # assign values to the column for t+1 prediction\n",
    "        df_out.at[len(df_out.index),['REPORT_DATE','AD_GROUP_ID','KEYWORD_ID','MATCH_TYPE',f'predicted_{target_feature}']] = [next_date,\n",
    "                                                                                                                              AD_GROUP_ID,\n",
    "                                                                                                                              KEYWORD_ID,\n",
    "                                                                                                                              MATCH_TYPE,\n",
    "                                                                                                                              y_pred[-1]]\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    def get_metrics_df(self,train_data, test_data, n_steps = (1,20)):\n",
    "        '''\n",
    "        Args :\n",
    "            train_data -> training dataframe\n",
    "            test_data -> test dataframe\n",
    "            n_steps -> tuple of n_steps to be used for training, \n",
    "                       model will be trained for each n_step value from the given range.\n",
    "        Returns :\n",
    "            It will return dataframe with diffrent n_step_values with corresponding metrics (RMSE, R2) of train and test data.\n",
    "        \n",
    "        Note : This method is used to find the best n_step value from the given n_steps range.\n",
    "        '''\n",
    "        N_steps = []\n",
    "        train_r2 = []\n",
    "        test_r2 = []\n",
    "        train_rmse = []\n",
    "        test_rmse = []\n",
    "        train_mape = []\n",
    "        test_mape = []\n",
    "\n",
    "        # iterate over specified n_steps values and store results in the above lists\n",
    "        for i in range(n_steps[0],n_steps[1]+1):\n",
    "            if i >= len(train_data) or i >= len(test_data):\n",
    "                continue\n",
    "            # train lstm model for the given n_step value in for loop\n",
    "            model1_metrics = self.lstm_model(train_data=train_data, test_data=test_data, n_steps=i)\n",
    "            # append current n_step and metrics to the above lists\n",
    "            N_steps.append(i)\n",
    "            train_r2.append(model1_metrics['train_R2'])\n",
    "            test_r2.append(model1_metrics['test_R2'])\n",
    "            train_rmse.append(model1_metrics['TRAIN_RMSE'])\n",
    "            test_rmse.append(model1_metrics['TEST_RMSE'])\n",
    "            train_mape.append(model1_metrics['TRAIN_MAPE'])\n",
    "            test_mape.append(model1_metrics['TEST_MAPE'])\n",
    "        \n",
    "        # return dataframe of metrics associated with each n_step value\n",
    "        return pd.DataFrame({\n",
    "            'n_steps' : N_steps,\n",
    "            'train_r2' : train_r2,\n",
    "            'test_r2' : test_r2,\n",
    "            'train_rmse' : train_rmse,\n",
    "            'test_rmse' : test_rmse,\n",
    "            'train_mape' : train_mape,\n",
    "            'test_mape' : test_mape\n",
    "        })\n",
    "\n",
    "    def tuned_lstm(self,train_data,test_data,n_steps):\n",
    "        '''\n",
    "        Args :\n",
    "            train_data -> train dataframe\n",
    "            test_data -> test dataframe\n",
    "            n_steps -> look back window size\n",
    "        Returns :\n",
    "            It will train LSTM model with given best n_step value and return dict of trained model object and metrics info.\n",
    "\n",
    "        Note : this method used to train lstm model with best found n_step value.\n",
    "        '''\n",
    "        \n",
    "        model_output = self.lstm_model(train_data=train_data, test_data=test_data, n_steps= n_steps)\n",
    "        return model_output\n",
    "\n",
    "    \n",
    "    def min_max_scaling(self,train_data, test_data):\n",
    "        '''\n",
    "        Args :\n",
    "            train_data -> train dataframe to be scaled\n",
    "            test_data -> test dataframe to be scaled\n",
    "        Returns :\n",
    "            Dictionary of scaled_train_data, scaled_test_data and scaler object\n",
    "        '''\n",
    "        df_columns = train_data.columns.tolist()\n",
    "        sc = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "        # fit transform on training dataframe\n",
    "        scaled_train_data = pd.DataFrame(sc.fit_transform(train_data),columns = df_columns)\n",
    "        # transform on test dataframe\n",
    "        scaled_test_data = pd.DataFrame(sc.transform(test_data),columns = df_columns)\n",
    "        \n",
    "        return {'scaled_train_data' : scaled_train_data,\n",
    "                'scaled_test_data' : scaled_test_data,\n",
    "                'scaler' : sc}\n",
    "\n",
    "    def keras_to_onnx_model(self,keras_model,best_n_steps,n_features,root_path,AD_GROUP_ID,KEYWORD_ID,MATCH_TYPE,target_feature):\n",
    "        # year\n",
    "        year = self.todays_date.strftime(\"%Y\")\n",
    "        # Month\n",
    "        month = self.todays_date.strftime(\"%m\")\n",
    "        # Day\n",
    "        day = self.todays_date.strftime(\"%d\")\n",
    "        # join string with current year, month and day\n",
    "        year_month_day =  str(year) + str(month) + str(day)\n",
    "        \n",
    "        # model name as a string of year_month_day_AD_GROUP_ID_KEYWORD_ID_MATCH_TYPE_{target_feature}Model\n",
    "        model_name = '_'.join([year_month_day,\n",
    "                               str(int(AD_GROUP_ID)),\n",
    "                               str(int(KEYWORD_ID)),\n",
    "                               str(MATCH_TYPE),\n",
    "                               f'{target_feature}',\n",
    "                               'LSTMModel'])\n",
    "        \n",
    "        full_path = os.path.join(root_path,model_name) + '.onnx'\n",
    "\n",
    "        ########### onnx model ########################\n",
    "        spec = (tf.TensorSpec((None,best_n_steps,n_features), tf.double, name=\"input\"),)\n",
    "        # output_path = 'output/20_oct/onnx_test/model2.onnx'\n",
    "        model_proto, _ = tf2onnx.convert.from_keras(keras_model, input_signature=spec, opset=13, output_path=full_path)\n",
    "        output_names = [n.name for n in model_proto.graph.output]\n",
    "        return output_names\n",
    "\n",
    "    def create_keras_layers(self,n_steps,n_features):\n",
    "        # define model(to decide best params hyperparameter tuning is required)\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(16, activation='relu', return_sequences=True,input_shape=(n_steps, n_features)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(16, activation='relu', return_sequences=False,input_shape=(n_steps, n_features)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def lstm_model(self,train_data,test_data,n_steps):\n",
    "        '''\n",
    "        Args :\n",
    "            train_data -> train dataframe\n",
    "            test_data -> test dataframe\n",
    "            n_steps -> look back window size\n",
    "        Returns :\n",
    "            It will return dictionary with trained model object and metrics values of train and test data.\n",
    "        '''\n",
    "        # convert into input/output (Traing data)\n",
    "        x_train, y_train = self.split_sequences(train_data, n_steps)\n",
    "\n",
    "        # convert into input/output (Test data)\n",
    "        x_test, y_test = self.split_sequences(test_data, n_steps)\n",
    "        \n",
    "        # convert into input/output (full data)\n",
    "        scaled_dataset_full = train_data.append(test_data)\n",
    "        \n",
    "        x_full = self.split_sequences_inference(scaled_dataset_full.iloc[:,:-1].values,n_steps)\n",
    "        \n",
    "        # the dataset knows the number of features, e.g. 2\n",
    "        n_features = x_train.shape[2]\n",
    "\n",
    "        # define model(to decide best params hyperparameter tuning is required)\n",
    "        # model = Sequential()\n",
    "        # model.add(LSTM(16, activation='relu', return_sequences=True,input_shape=(n_steps, n_features)))\n",
    "        # model.add(Dropout(0.2))\n",
    "        # model.add(LSTM(16, activation='relu', return_sequences=False,input_shape=(n_steps, n_features)))\n",
    "        # model.add(Dropout(0.2))\n",
    "        # model.add(Dense(1))\n",
    "        # model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        model = self.create_keras_layers(n_steps, n_features)\n",
    "        # fit model\n",
    "        model.fit(x_train, y_train, epochs=50,batch_size=1,verbose=0)\n",
    "\n",
    "        \n",
    "        # demonstrate prediction on test data\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        # get prediction on training data\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        # get prediction on full data\n",
    "        y_full_pred = model.predict(x_full)[:,0]\n",
    "        \n",
    "        # calculate root mean squared error for train and test data\n",
    "        train_RMSE = np.sqrt(mean_squared_error(y_train, y_train_pred[:,0]))\n",
    "        test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred[:,0]))\n",
    "        # calculate R2 score for train and test data\n",
    "        train_r2 = r2_score(y_train,y_train_pred[:,0])\n",
    "        test_r2 = r2_score(y_test,y_test_pred[:,0])\n",
    "        # calculate mean absolute percentage error for train and test data\n",
    "        train_MAPE = mean_absolute_percentage_error(y_train, y_train_pred[:,0])\n",
    "        test_MAPE = mean_absolute_percentage_error(y_test, y_test_pred[:,0])\n",
    "        # calculate symmetric mean absolute percentage error for train and test data\n",
    "        train_smape_datalength, train_smape_skippedcnt, train_SMAPE = self.smape(y_train, y_train_pred[:,0])\n",
    "        test_smape_datalength, test_smape_skippedcnt, test_SMAPE = self.smape(y_test, y_test_pred[:,0])\n",
    "        \n",
    "        # to overcome the issue of nan value round off of metrics \n",
    "        if not np.isnan(train_RMSE):\n",
    "            train_RMSE = round(train_RMSE,2)     \n",
    "        if not np.isnan(test_RMSE):\n",
    "            test_RMSE = round(test_RMSE,2)\n",
    "        if not np.isnan(train_MAPE):\n",
    "            train_MAPE = round(train_MAPE,2)     \n",
    "        if not np.isnan(test_MAPE):\n",
    "            test_MAPE = round(test_MAPE,2)\n",
    "        if not np.isnan(train_SMAPE):\n",
    "            train_SMAPE = round(train_SMAPE,2)     \n",
    "        if not np.isnan(test_SMAPE):\n",
    "            test_SMAPE = round(test_SMAPE,2)\n",
    "        if not np.isnan(train_r2):\n",
    "            train_r2 = round(train_r2,2)     \n",
    "        if not np.isnan(test_r2):\n",
    "            test_r2 = round(test_r2,2)\n",
    "\n",
    "        return {'MODEL' : model,\n",
    "                'TRAIN_RMSE': train_RMSE, \n",
    "                'TEST_RMSE' : test_RMSE,\n",
    "                'TRAIN_MAPE' : train_MAPE,\n",
    "                'TEST_MAPE' : test_MAPE,\n",
    "                'TRAIN_SMAPE' : train_SMAPE,\n",
    "                'TEST_SMAPE' : test_SMAPE,\n",
    "                'train_R2' : train_r2, \n",
    "                'test_R2' : test_r2,\n",
    "                'Y_TEST' :  y_test.round(2),\n",
    "                'Y_TEST_PRED' : y_test_pred[:,0].round(2),\n",
    "                'Y_PRED_FULL' : y_full_pred.round(2),\n",
    "                # added on 19th october for comparision purpose\n",
    "                'train_smape_datalength' : train_smape_datalength,\n",
    "                'test_smape_datalength' :test_smape_datalength}\n",
    "\n",
    "    def lstm_model_training_full(self,data,n_steps):\n",
    "        '''\n",
    "        Args :\n",
    "            data -> full dataframe\n",
    "            n_steps -> look back window size\n",
    "        Returns :\n",
    "            It will return trained model object on 100% data.\n",
    "        '''\n",
    "        # convert into input/output (Full data)\n",
    "        x_train, y_train = self.split_sequences(data, n_steps)\n",
    "\n",
    "        # the dataset knows the number of features, e.g. 2\n",
    "        n_features = x_train.shape[2]\n",
    "\n",
    "        # define model(to decide best params hyperparameter tuning is required)\n",
    "        lstm_model_full_Data = self.create_keras_layers(n_steps, n_features)\n",
    "        # fit model\n",
    "        lstm_model_full_Data.fit(x_train, y_train, epochs=50,batch_size=1,verbose=0)\n",
    "\n",
    "        return lstm_model_full_Data\n",
    "        \n",
    "    def tsa_model(self,df,independent_features,target_feature,n_steps,train_size,root_path,serialize_model=None):\n",
    "        '''\n",
    "        Args:\n",
    "            df -> input dataframe\n",
    "            independent_features -> list of independent feature\n",
    "            target_feature -> name of the target feature\n",
    "            n_steps -> tuple of n_steps i.e (1,5)\n",
    "            train_size -> train_size between 0 to 1 (i.e 0.70)\n",
    "            root_path -> path to store the trained model\n",
    "            serialize_model -> flag True/False, if True then model will be stored at given path\n",
    "        Returns:\n",
    "            -> It will train lstm model by finding best n_step value from the given range of n_steps and\n",
    "               serialize model and dump at given location if serialize_model = True.\n",
    "        '''\n",
    "        features = independent_features.copy()\n",
    "        features.append(target_feature)\n",
    "        # preprocess dataframe with independent + target features\n",
    "        df = self.data_preprocessing(df,features)\n",
    "        # create copy of the df with independent + target features\n",
    "        dataset = df[features].copy()\n",
    "        \n",
    "        # get train test data\n",
    "        train_data,test_data = self.train_test_split(dataset,train_size)\n",
    "        \n",
    "        # added to overcome the issue of target feature being present as both independent feature and target feature\n",
    "        if target_feature in independent_features:\n",
    "            # take only independdent feature from train/test data and exclude target feature\n",
    "            train_data = train_data.iloc[:,:-1]\n",
    "            test_data = test_data.iloc[:,:-1]\n",
    "        \n",
    "        # scale train and test data\n",
    "        features_to_scale = independent_features.copy()\n",
    "        #print('\\n','Features to scale before',features_to_scale)\n",
    "        if 'ROAS_TARGET' in independent_features:\n",
    "            # get index of ROAS_TARGET to append back in later stage\n",
    "            ROAS_TARGET_index = independent_features.index('ROAS_TARGET')\n",
    "            features_to_scale.remove('ROAS_TARGET') # log transformation is done on ROAS_TARGET so we are removing it from scaling\n",
    "            \n",
    "        #print('Features to scale : ',features_to_scale)\n",
    "        \n",
    "        # take only those feature for scaling which are in features_to_scale list\n",
    "        train_data_to_scale = train_data[features_to_scale] \n",
    "        test_data_to_scale = test_data[features_to_scale]\n",
    "        \n",
    "        # scale train and test data and return dictionary of  scaled train data, scaled test data and scaler \n",
    "        scaled_output = self.min_max_scaling(train_data_to_scale,test_data_to_scale)\n",
    "\n",
    "        # print('train scaled output columns before appending ROAS and target','\\n',scaled_output['scaled_train_data'].columns)\n",
    "        # print('test scaled output columns before appending ROAS and target','\\n',scaled_output['scaled_test_data'].columns)\n",
    "        \n",
    "        if 'ROAS_TARGET' in independent_features:\n",
    "            # append back non scaled ROAS_TARGET to the df if it is in independent features\n",
    "            scaled_output['scaled_train_data'].insert(loc=ROAS_TARGET_index,\n",
    "                                                    column='ROAS_TARGET',\n",
    "                                                    value=train_data['ROAS_TARGET'])\n",
    "            \n",
    "            scaled_output['scaled_test_data'].insert(loc=ROAS_TARGET_index,\n",
    "                                                    column='ROAS_TARGET',\n",
    "                                                    value=train_data['ROAS_TARGET'])\n",
    "        \n",
    "    \n",
    "        # append back non scaled target feature\n",
    "        scaled_output['scaled_train_data'].insert(loc=scaled_output['scaled_train_data'].shape[1],\n",
    "                                                  column=target_feature,\n",
    "                                                  value=train_data[target_feature],\n",
    "                                                  allow_duplicates=True)\n",
    "        \n",
    "        scaled_output['scaled_test_data'].insert(loc=scaled_output['scaled_test_data'].shape[1],\n",
    "                                                  column=target_feature,\n",
    "                                                  value=train_data[target_feature],\n",
    "                                                  allow_duplicates=True)\n",
    "        #print('\\n Fetures of the scaled_train_data \\n',scaled_output['scaled_train_data'].columns,scaled_output['scaled_train_data'].shape)\n",
    "        #print('\\n Fetures of the scaled_test_data \\n',scaled_output['scaled_test_data'].columns, scaled_output['scaled_test_data'].shape)\n",
    "        \n",
    "        # get the dataframe of metric values associated with each n_step value\n",
    "        metric_df_ = self.get_metrics_df(train_data=scaled_output['scaled_train_data'],\n",
    "                                         test_data=scaled_output['scaled_test_data'],\n",
    "                                         n_steps = n_steps)\n",
    "        \n",
    "        # get the best n_step value by sorting test_rmse column in ascending order\n",
    "        best_n_steps = int(metric_df_.sort_values('test_rmse').iloc[0]['n_steps'])\n",
    "        \n",
    "        # use best_n_steps value from above and get output from tuned_lstm model on 80-20 split data\n",
    "        model_output = self.tuned_lstm(train_data=scaled_output['scaled_train_data'],\n",
    "                                       test_data=scaled_output['scaled_test_data'],\n",
    "                                       n_steps= int(best_n_steps))\n",
    "        \n",
    "        ############## get trained model object from 100% data ##########################\n",
    "        full_scaled_data = scaled_output['scaled_train_data'].append(scaled_output['scaled_test_data'])\n",
    "        #print('shape of full_sclaed_data = ',full_scaled_data.shape)\n",
    "        trained_model_on_full_data = self.lstm_model_training_full(full_scaled_data,best_n_steps)\n",
    "\n",
    "        AD_GROUP_ID = df['AD_GROUP_ID'].iloc[0]\n",
    "        KEYWORD_ID = df['KEYWORD_ID'].iloc[0]\n",
    "        MATCH_TYPE = df['MATCH_TYPE'].iloc[0]\n",
    "        \n",
    "        # print AKM with RMSE, MAPE, SMAPE, R2 for train and test data\n",
    "        print('\\n',f'AD_GROUP_ID = {AD_GROUP_ID}, KEYWORD_ID = {KEYWORD_ID}, MATCH_TYPE = {MATCH_TYPE}','\\n')\n",
    "        print(f'Best n_steps: {int(best_n_steps)}')\n",
    "        print('Train Score:  RMSE = %.2f, MAPE = %.2f, SMAPE = %.2f, R2 = %.2f' % (model_output['TRAIN_RMSE'], \n",
    "                                                                                   model_output['TRAIN_MAPE'],\n",
    "                                                                                   model_output['TRAIN_SMAPE'],\n",
    "                                                                                   model_output['train_R2']))\n",
    "        print('Test Score: RMSE = %.2f, MAPE = %.2f, SMAPE = %.2f, R2 = %.2f' % (model_output['TEST_RMSE'], \n",
    "                                                                                 model_output['TEST_MAPE'],\n",
    "                                                                                 model_output['TEST_SMAPE'],\n",
    "                                                                                 model_output['test_R2']))\n",
    "        \n",
    "        print('\\n','-'*100,'\\n')\n",
    "        \n",
    "        # create dataframe of AKM values along with scaler object, best n_steps, RMSE, MAPE, SMAPE and R2 metrics\n",
    "        metric_df = pd.DataFrame({'AD_GROUP_ID' : [AD_GROUP_ID], \n",
    "                       'KEYWORD_ID': [KEYWORD_ID], \n",
    "                       'MATCH_TYPE': [MATCH_TYPE],\n",
    "                       'SCALER' : [scaled_output['scaler']],\n",
    "                       'BEST_N_STEPS' : [best_n_steps],\n",
    "                       'TRAIN_RMSE' : [model_output['TRAIN_RMSE']],\n",
    "                       'TEST_RMSE' : [model_output['TEST_RMSE']],\n",
    "                       'TRAIN_MAPE' : [model_output['TRAIN_MAPE']],\n",
    "                       'TEST_MAPE' : [model_output['TEST_MAPE']],\n",
    "                       'TRAIN_SMAPE' : [model_output['TRAIN_SMAPE']],\n",
    "                       'TEST_SMAPE' : [model_output['TEST_SMAPE']],\n",
    "                       'TRAIN_R2' : [model_output['train_R2']],\n",
    "                       'TEST_R2' : [model_output['test_R2']]})\n",
    "        \n",
    "        # added on 19th october\n",
    "        # this dataframe is temporory only for comparision with knn person\n",
    "        df_metrics_model_comparision = pd.DataFrame({'akm' : str(AD_GROUP_ID)+'-'+str(KEYWORD_ID)+'-'+str(MATCH_TYPE),\n",
    "                                                     'total_record_post_processed' : len(df),\n",
    "                                                     'best_n_steps' : [best_n_steps],\n",
    "                                                     'test_smape_lstm' : [model_output['TEST_SMAPE']],\n",
    "                                                     'best_n_step_df_total_records' : model_output['train_smape_datalength'] + model_output['test_smape_datalength'],\n",
    "                                                     'best_n_step_df_train_records' : model_output['train_smape_datalength'],\n",
    "                                                     'best_n_step_df_test_records' :  model_output['test_smape_datalength']\n",
    "                                                     })\n",
    "        \n",
    "        # create dataframe of the y_test real values and y_test predicted values\n",
    "        y_test_and_pred = pd.DataFrame({f'{target_feature}_test' : model_output['Y_TEST'],\n",
    "                                        f'{target_feature}_pred' : model_output['Y_TEST_PRED']})\n",
    "\n",
    "        if serialize_model :\n",
    "            #serialize model on 80 % data\n",
    "            # onnx_output_names = self.keras_to_onnx_model(keras_model=model_output['MODEL'],\n",
    "            #                                              best_n_steps=int(best_n_steps),\n",
    "            #                                              n_features=(scaled_output['scaled_train_data'].shape[1])-1,\n",
    "            #                                              root_path=root_path,\n",
    "            #                                              AD_GROUP_ID=AD_GROUP_ID,\n",
    "            #                                              KEYWORD_ID=KEYWORD_ID,\n",
    "            #                                              MATCH_TYPE=MATCH_TYPE,\n",
    "            #                                              target_feature=target_feature)\n",
    "            \n",
    "            # serialize model on 100% data\n",
    "            onnx_output_names = self.keras_to_onnx_model(keras_model=trained_model_on_full_data,\n",
    "                                                         best_n_steps=int(best_n_steps),\n",
    "                                                         n_features=len(independent_features),\n",
    "                                                         root_path=root_path,\n",
    "                                                         AD_GROUP_ID=AD_GROUP_ID,\n",
    "                                                         KEYWORD_ID=KEYWORD_ID,\n",
    "                                                         MATCH_TYPE=MATCH_TYPE,\n",
    "                                                         target_feature=target_feature)\n",
    "            \n",
    "        \n",
    "        # append predicted result to the original dataframe\n",
    "        output_df = self.append_pred_result(df,model_output['Y_PRED_FULL'],best_n_steps,target_feature)\n",
    "        # append delta change\n",
    "        out_df = self.rolling_diff(output_df,target_feature,best_n_steps)\n",
    "        \n",
    "        return out_df, y_test_and_pred, metric_df, df_metrics_model_comparision,onnx_output_names # added on 20th october\n",
    "\n",
    "    def model_1(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            It will use tsa_model() method to train model by using model_1 independent_feature and target feature \n",
    "            and return dataframe with prediction appended to it, y_test and y_test_pred dataframe and metrics df. \n",
    "        '''\n",
    "        try:\n",
    "            # create copy of the input dataframe\n",
    "            df = self.df.copy()\n",
    "            # make REPORT_DATE as datetime dtype\n",
    "            df[\"REPORT_DATE\"]= pd.to_datetime(df[\"REPORT_DATE\"],yearfirst=True)\n",
    "            # sort dataframe by report_date\n",
    "            df = df.sort_values(by='REPORT_DATE').reset_index(drop=True)\n",
    "            \n",
    "            inference_date = df.iloc[-1]['REPORT_DATE'] + pd.Timedelta(days=1) # this is just for recording training inference date as statistic\n",
    "            independent_features = self.model_1_independent_features.copy()    # make a copy of the independent features list\n",
    "            \n",
    "            # if ROAS_TARGET is in independet features then create and assign ROAS_TARGET feature to the dataframe\n",
    "            # if 'ROAS_TARGET' in independent_features:\n",
    "            #     # create ROAS feature\n",
    "            #     df['ROAS_TARGET'] = np.log(((df['SALES_USD']+1)/(df['SPEND_USD']+1)))\n",
    "            \n",
    "            \n",
    "            target_feature = self.model_1_target_feature\n",
    "            n_steps = self.n_steps\n",
    "            train_size = self.train_size\n",
    "            root_path = self.root_path \n",
    "            #print('\\n','MODEL 1 Independent Features :',independent_features)\n",
    "            \n",
    "           \n",
    "            model_1_output, model1_test_and_pred, model_1_metric_df,df_metrics_model_comparision,onnx_output_names = self.tsa_model(df=df,\n",
    "                                                                                                                    independent_features = independent_features,\n",
    "                                                                                                                    target_feature = target_feature, \n",
    "                                                                                                                    n_steps = n_steps,\n",
    "                                                                                                                    train_size = train_size,\n",
    "                                                                                                                    root_path = root_path,\n",
    "                                                                                                                    serialize_model = True)\n",
    "            \n",
    "            # serialize akm and best n_steps from each model\n",
    "            self.serialize_akm_metadata_model_1(root_path,model_1_output,model_1_metric_df,independent_features,inference_date,onnx_output_names)\n",
    "        \n",
    "            #return model_1_output, model1_test_and_pred, model_1_metric_df\n",
    "            return model_1_output, model1_test_and_pred, model_1_metric_df, df_metrics_model_comparision # added on 19th october for comparision\n",
    "\n",
    "        except:\n",
    "            AD_GROUP_ID = df.iloc[0]['AD_GROUP_ID']\n",
    "            KEYWORD_ID = df.iloc[0]['KEYWORD_ID']\n",
    "            MATCH_TYPE = df.iloc[0]['MATCH_TYPE']\n",
    "            print('*'*100)\n",
    "            print('\\n',f'{target_feature}_MODEL training failed for AD_GROUP_ID = {AD_GROUP_ID}, KEYWORD_ID = {KEYWORD_ID}, MATCH_TYPE = {MATCH_TYPE}','\\n')\n",
    "            print(traceback.format_exc())\n",
    "            print('*'*100)\n",
    "        \n",
    "       \n",
    "    def main(self):\n",
    "        try:\n",
    "            bid_model_output, bid_model_test_and_pred, metric_df, df_metrics_model_comparision = self.model_1()\n",
    "            return bid_model_output, bid_model_test_and_pred, metric_df, df_metrics_model_comparision\n",
    "        except:\n",
    "            print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJTClUKsU5is"
   },
   "source": [
    "# Train model for multiple AKMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xxJh9hewU449"
   },
   "outputs": [],
   "source": [
    "def run_lstm_training(input_path,akm_to_model_path,independent_features,target_feature,train_size,root_path,n_steps):\n",
    "    '''\n",
    "    Args:\n",
    "        input_path -> input file path with all akm\n",
    "        akm_to_model_path -> Csv of the AD_GROUP_ID, KEYWORD_ID and MATCH_TYPE to train LSTM models only\n",
    "        independent_features -> Independent feature list\n",
    "        target_feature -> target feature name\n",
    "        train_size -> train size (eg. 0.70)\n",
    "        root_path -> Path to store LSTM model\n",
    "        n_steps -> tupe of n_steps (eg. (1,2))\n",
    "    Returns:\n",
    "        This method is used to run training for n number of akm and return dataframes .\n",
    "    '''\n",
    "    df = pd.read_csv(input_path,parse_dates=['REPORT_DATE']) # read full csv\n",
    "    df_akm_to_model = pd.read_csv(akm_to_model_path)       # read akm_to_model csv\n",
    "\n",
    "    def get_records_with_akm_combinations(df, AD_GROUP_ID, KEYWORD_ID, MATCH_TYPE):\n",
    "        df = df.loc[(df['AD_GROUP_ID'] == AD_GROUP_ID) & (df['KEYWORD_ID'] == KEYWORD_ID) & \n",
    "                    (df['MATCH_TYPE'] == MATCH_TYPE)].reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def isEnoughData(df,column):\n",
    "        for col in column:\n",
    "            distcnt = np.count_nonzero(df[col], axis=0)\n",
    "            if (distcnt < 10):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # # this function will remove rows which have all zeros/NaN for given feature list\n",
    "    def data_sufficiency_check(df,features):\n",
    "        df = df.loc[(df[features] != 0).any(axis=1)].dropna(subset=features).reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    insuff_akm_count = 0\n",
    "    failed_count = 0\n",
    "    comparision_metric_df = pd.DataFrame()\n",
    "    bid_model_metric_df = pd.DataFrame()\n",
    "\n",
    "    for idx, row in df_akm_to_model.iterrows():\n",
    "        try:\n",
    "            print(f'\\n ************************** Iteration {idx+1} ************************************ \\n')\n",
    "            AD_GROUP_ID, KEYWORD_ID, MATCH_TYPE = row['AD_GROUP_ID'], row['KEYWORD_ID'], row['MATCH_TYPE']\n",
    "            akm = str(AD_GROUP_ID)+'-'+str(KEYWORD_ID)+'-'+str(MATCH_TYPE)\n",
    "            # testakm = '157188419817864-177620140515274-PHRASE'\n",
    "            # if (akm != testakm):\n",
    "            #     continue\n",
    "            df_akm = get_records_with_akm_combinations(df,\n",
    "                                                    AD_GROUP_ID =AD_GROUP_ID,\n",
    "                                                    KEYWORD_ID=KEYWORD_ID,\n",
    "                                                    MATCH_TYPE=MATCH_TYPE)\n",
    "            \n",
    "            if 'ROAS_TARGET' in independent_features:\n",
    "                df_akm['ROAS_TARGET'] = np.log(((df_akm['SALES_USD']+1)/(df_akm['SPEND_USD']+1)))\n",
    "                    \n",
    "            df_akm = data_sufficiency_check(df_akm,independent_features+[target_feature]) # df_akm is processed and it will be used further\n",
    "\n",
    "            print('df_akm shape = ', df_akm.shape, 'isEnoughData flag = ',isEnoughData(df_akm,[target_feature]))\n",
    "        \n",
    "            if not isEnoughData(df_akm,[target_feature]):\n",
    "                # print('*'*100)\n",
    "                print(f'\\n ***** Insufficient data for AD_GROUP_ID = {AD_GROUP_ID},KEYWORD_ID = {KEYWORD_ID}, MATCH_TYPE = {MATCH_TYPE} ***** \\n')\n",
    "                # print('*'*100)\n",
    "                insuff_akm_count+=1\n",
    "                continue\n",
    "\n",
    "            if (len(df_akm) <= 10):\n",
    "                # print('*'*100)\n",
    "                print(f'\\n ***** Insufficient data for AD_GROUP_ID = {AD_GROUP_ID},KEYWORD_ID = {KEYWORD_ID}, MATCH_TYPE = {MATCH_TYPE} ***** \\n')\n",
    "                # print('*'*100)\n",
    "                insuff_akm_count+=1\n",
    "                continue\n",
    "\n",
    "            lstm = LSTM_MODEL(df = df_akm,\n",
    "                        model_1_independent_features = independent_features,\n",
    "                        model_1_target_feature = target_feature,\n",
    "                        n_steps = n_steps,\n",
    "                        train_size = train_size,\n",
    "                        root_path = root_path)\n",
    "            \n",
    "            bid_model_output, bid_model_test_and_pred, bid_model_metric_df, df_metrics_model_comparision = lstm.main()\n",
    "            comparision_metric_df = comparision_metric_df.append(df_metrics_model_comparision)\n",
    "            bid_model_metric_df = bid_model_metric_df.append(bid_model_metric_df)\n",
    "\n",
    "        except:\n",
    "            failed_count+=1\n",
    "            print('*'*100)\n",
    "            print('\\n',f'Training failed for AD_GROUP_ID = {AD_GROUP_ID}, KEYWORD_ID = {KEYWORD_ID}, MATCH_TYPE = {MATCH_TYPE}','\\n')\n",
    "            print(traceback.format_exc())\n",
    "            print('*'*100)\n",
    "            pass\n",
    "\n",
    "    print(f'\\n ***** Total skipped AKM combination due to insufficient data = {insuff_akm_count} ***** \\n')\n",
    "    print(f'\\n ***** Total Failed AKM combination = {failed_count} ***** \\n')\n",
    "\n",
    "    return bid_model_output, bid_model_test_and_pred, bid_model_metric_df, comparision_metric_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WN3zkt5FjUmn"
   },
   "outputs": [],
   "source": [
    "input_path='akmtrainingdata.csv'\n",
    "akm_to_model_path = 'akm.csv'\n",
    "train_size = 0.80\n",
    "root_path = 'LSTMModels'\n",
    "n_steps=(1,15)\n",
    "independent_features = ['CPC_USD','SALES_USD','CONVERSIONS','CLICKS','ROAS_TARGET','IMPRESSIONS','SPEND_USD']\n",
    "target_feature='CPC_USD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429824,
     "status": "ok",
     "timestamp": 1667549810222,
     "user": {
      "displayName": "Sumit Tagadiya",
      "userId": "10665247559153427451"
     },
     "user_tz": -330
    },
    "id": "enMVM7G5MjXE",
    "outputId": "9c5fe551-6bd5-4089-c4ed-d8cdb32f480a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************************** Iteration 1 ************************************ \n",
      "\n",
      "df_akm shape =  (100, 56) isEnoughData flag =  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 19:11:52.231648: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-15 19:11:52.231701: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-15 19:11:52.231728: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (beejay-ThinkPad): /proc/driver/nvidia/version does not exist\n",
      "2022-11-15 19:11:52.232031: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AD_GROUP_ID = 157188419817864, KEYWORD_ID = 177620140515274, MATCH_TYPE = PHRASE \n",
      "\n",
      "Best n_steps: 13\n",
      "Train Score:  RMSE = 0.24, MAPE = 0.07, SMAPE = 7.43, R2 = 0.53\n",
      "Test Score: RMSE = 0.55, MAPE = 0.19, SMAPE = 17.05, R2 = -197.94\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "WARNING:tensorflow:From /home/beejay/anaconda3/envs/heydaypy3104/lib/python3.10/site-packages/tf2onnx/tf_loader.py:557: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 19:16:04.065598: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-11-15 19:16:04.065749: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-11-15 19:16:04.076451: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 140 nodes (0), 171 edges (0), time = 1.74ms.\n",
      "  function_optimizer: Graph size after: 140 nodes (0), 171 edges (0), time = 1.85ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_33_while_cond_239620\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_32_while_body_239481\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_32_while_cond_239480\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_33_while_body_239621\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2022-11-15 19:16:04.194391: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-11-15 19:16:04.194592: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-11-15 19:16:04.221791: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 88 nodes (-2), 99 edges (-8), time = 4.256ms.\n",
      "  function_optimizer: Graph size after: 88 nodes (0), 99 edges (0), time = 3.024ms.\n",
      "  constant_folding: Graph size after: 88 nodes (0), 99 edges (0), time = 2.801ms.\n",
      "  function_optimizer: Graph size after: 88 nodes (0), 99 edges (0), time = 2.922ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_32_while_cond_frozen_239729\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.402ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.208ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_32_while_body_frozen_239730\n",
      "  constant_folding: Graph size after: 54 nodes (0), 62 edges (0), time = 1.108ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  constant_folding: Graph size after: 54 nodes (0), 62 edges (0), time = 0.732ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_33_while_cond_frozen_239731\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.285ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.198ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: sequential_16_lstm_33_while_body_frozen_239732\n",
      "  constant_folding: Graph size after: 54 nodes (0), 62 edges (0), time = 0.852ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  constant_folding: Graph size after: 54 nodes (0), 62 edges (0), time = 0.762ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKM metadata Dictionary saved at LSTMModels/20221115_157188419817864_177620140515274_PHRASE_CPC_USD_LSTMModel\n",
      "\n",
      " ***** Total skipped AKM combination due to insufficient data = 0 ***** \n",
      "\n",
      "\n",
      " ***** Total Failed AKM combination = 0 ***** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bid_model_output, bid_model_test_and_pred, bid_model_metric_df,comparision_metric_df  = run_lstm_training(input_path=input_path,\n",
    "                                                                                                  akm_to_model_path=akm_to_model_path,\n",
    "                                                                                                  independent_features=independent_features,\n",
    "                                                                                                  target_feature=target_feature,\n",
    "                                                                                                  train_size=train_size,\n",
    "                                                                                                  root_path=root_path,\n",
    "                                                                                                  n_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "10wko26tpU4EgvRBgSEXO09KMTXyvXuP_",
     "timestamp": 1664397800174
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
