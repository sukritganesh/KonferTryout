{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zLTaybv5dNHG","executionInfo":{"status":"ok","timestamp":1668896340289,"user_tz":480,"elapsed":12751,"user":{"displayName":"Sukrit Ganesh","userId":"09882607507987906814"}}},"outputs":[],"source":["# from pgmpy.inference import VariableElimination\n","# from pgmpy.models import BayesianNetwork\n","import math\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import PercentFormatter\n","import time\n","from statsmodels.tsa.stattools import grangercausalitytests\n","\n","import os,sys\n","from datetime import datetime\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from numpy import array\n","import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense, Dropout\n","import tensorflow as tf\n","import joblib\n","from joblib import Parallel, delayed\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n","from keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import MinMaxScaler\n","import multiprocessing\n","import seaborn as sns\n","import traceback\n","import glob\n","#import bnlearn as bn\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from collections import OrderedDict\n","from sklearn.model_selection import LeaveOneOut\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nawJQxIGklhI","executionInfo":{"status":"ok","timestamp":1668896351158,"user_tz":480,"elapsed":923,"user":{"displayName":"Sukrit Ganesh","userId":"09882607507987906814"}}},"outputs":[],"source":["class KnnModelTrain:\n","    '''\n","    Attributes\n","    ----------------------------------------------------------------------------\n","    input_path : Pandas DataFrame\n","        -> Path of the full csv file\n","    akm_to_model_path : str\n","        -> Path of the akm_to_model for KNN csv file\n","    logfile_path\n","        -> Path where logfile to be stored\n","    model_path\n","        -> Path where trained KNN models/jsons to be stored\n","    independent_features\n","        -> List of independent features for training\n","    target_feature\n","        -> Target feature name\n","\n","    Methods\n","    ----------------------------------------------------------------------------\n","    readcsv(file)\n","        -> It reads csv and return dataframe with sorted values by AKM and REPORT_DATE\n","\n","    getakmdata(fulldf,ad,kw,mt)\n","        -> It returns dataframe for the given akm values from the fulldf\n","\n","    isEnoughData(df,columns)\n","        -> It return True/False based on distinct value count of the target feature\n","    \n","    computeSMAPENoExp(act,pred)\n","        -> It returns smape value from the from the actual and predicted values\n","\n","    buildLaggedData(akm,akmdf,features,target, maxlag)\n","        -> It returns dictionary of LagToData and LagToFeatures \n","        LagToData ->  It will have Lag as key and corresponding dataframe as value\n","        LagToFeatures ->It will have Lag as key and corresponding feature list as value\n","\n","    trainModelAndLOOCVTest(akm,LagToData,LagToFeatures,target,objective,successfile)\n","        -> It returns Best lag dataframe, Best lag value and Best lag features list\n","        \n","    pyplotscatter(akm,target,actual,predicted,setrange)\n","        -> It will plot scattered plot of actual and predicted values with regression line\n","\n","    get_stats(df,feature)\n","        -> It will return  dictionary of 'min','max','mean' and 'std' value of the target feature\n","\n","    serialize_model(model,root_path,AD_GROUP_ID,KEYWORD_ID,MATCH_TYPE)\n","        -> It will serialize model for the given AKM combination and store it at root_path\n","\n","    serialize_akm_metadata(args)\n","        -> It will store akm metadata as json in binary format.\n","\n","    finalKNNModelTrainAndTest(akm,bestLag,bestLagData,bestFeatures,target,testsize,successfile)\n","        -> it will train KNN model on 80% data and test on 20% data and returns dictionary of \n","           'bestLag' ,'smapedatalength', 'skippedcnt', 'testsmape'\n","\n","    finalKNNModel(bestLagData,bestFeatures,target)\n","        -> It will return trained KNN model object on 100% data\n","\n","    trainModels(input_path,akm_to_model_path,logfile_path,model_path)\n","        -> It will train models for each AKM combinatiion from the akm_to_model file and store it at given path\n","\n","    '''\n","    def __init__(self,input_path,akm_to_model_path,logfile_path,model_path,independent_features,target_feature,max_lag,knnweight):\n","        self.input_path = input_path\n","        self.akm_to_model_path = akm_to_model_path\n","        self.logfile_path = logfile_path\n","        self.model_path = model_path\n","        self.independent_features = independent_features #independent features\n","        self.target_feature = target_feature\n","        self.max_lag = max_lag\n","        self.todays_date = datetime.now()\n","        self.knnweight = knnweight\n","        \n","    def readcsv(self,file):\n","        '''\n","        Args:\n","            file -> Input file path to read csv file\n","        Returns:\n","            It reads csv and return dataframe with sorted values by AKM and REPORT_DATE\n","        '''\n","        fullakmdf = pd.read_csv(file,parse_dates=['REPORT_DATE'])\n","        fullakmdf.sort_values(by=['AD_GROUP_ID','KEYWORD_ID','MATCH_TYPE','REPORT_DATE'])\n","        return fullakmdf\n","\n","    def getakmdata(self,fulldf,ad,kw,mt):\n","        '''\n","        Args:\n","            fulldf -> full dataframe with all akms\n","            ad -> AD_GROUP_ID value\n","            kw -> KEYWORD_ID value\n","            mt -> MATCH_TYPE value\n","        Returns:\n","            Return dataframe for the given akm combination\n","        '''\n","        akmdf = fulldf.loc[(fulldf['AD_GROUP_ID'] == ad) & (fulldf['KEYWORD_ID'] == kw) & (fulldf['MATCH_TYPE'] == mt)].reset_index(drop=True)\n","        akmdf = akmdf.sort_values(by=['REPORT_DATE']).reset_index(drop=True)\n","        return akmdf\n","\n","    def isEnoughData(self,df,columns):\n","        '''\n","        Args:\n","            df -> input dataframe\n","            columns -> target feature name(list)\n","        Returns:\n","            It return True/False based on distinct value count of the target feature\n","        '''\n","        for col in columns:\n","            distcnt = np.count_nonzero(df[col], axis=0)\n","            if (distcnt < 10):\n","                return False\n","            # sd = df[col].std()\n","        return True\n","\n","    def computeSMAPENoExp(self,act,pred):\n","        '''\n","        Args :\n","            act -> array/series/list of actual values \n","            pred -> array/series/list of predicted values\n","        Returns :\n","            It return symetric mean absolute percentage error\n","\n","        Assumption: actuals and predictions are not logarithms\n","        '''\n","         \n","        #Assumption: actuals and predictions are not logarithms\n","        smape = 0\n","        skippedcnt = 0\n","        for a,p in zip(act,pred):\n","            den = abs(a) + abs(p)\n","            if (den > 0):\n","                num = 2*abs(p-a)\n","            else:\n","                den = 1\n","                num = 2*abs(p-a)\n","                skippedcnt += 1\n","                \n","            smape += (num/den)\n","        \n","        smape = (100*smape)/len(act)\n","        smapedatalength = len(act)\n","                            \n","        return smapedatalength, skippedcnt, round(smape,2)\n","\n","    def buildLaggedData(self,akm,akmdf,features,target, maxlag):\n","        '''\n","        Args:\n","            akm -> akm string \n","            akmdf -> akm dataframe\n","            features -> independent feature list\n","            target -> target feature name\n","            maxlag -> maxlag value\n","        Returns:\n","            -> It returns dictionary of LagToData and LagToFeatures \n","            LagToData ->  It will have Lag as key and corresponding dataframe as value\n","            LagToFeatures ->It will have Lag as key and corresponding feature list as value\n","        '''\n","        LagToData = OrderedDict()\n","        LagToFeatures = OrderedDict()\n","        \n","        for lag in range(1,maxlag+1,1):\n","            if (lag == 1):\n","                #for first lag, create new dataframe and add target column data\n","                df = pd.DataFrame()\n","                df[target] = akmdf[target]\n","                newfeatures = []\n","            else:\n","                #for other lags lag, start with the previous lag's data\n","                df = LagToData[lag-1].copy(deep=True)\n","                newfeatures = [x for x in LagToFeatures[lag-1]]\n","            \n","            #Create lagged features for the given features\n","            for col in features:\n","                newcol = col+'_lag'+str(lag)\n","                df[newcol] = akmdf[col].shift(lag)\n","                newfeatures.append(newcol)\n","            \n","            #create lagged features for the target\n","            newcol = target+'_lag'+str(lag)\n","            df[newcol] = akmdf[target].shift(lag)\n","            newfeatures.append(newcol)\n","            \n","            #delete first row because it has missing features from lagging.\n","            #This is cumulative for the new DFs for new lags\n","            df = df.iloc[1:,:]\n","            \n","            #Add data and feature names to the dictionary\n","            LagToData[lag] = df\n","            LagToFeatures[lag] = newfeatures\n","            \n","        return LagToData, LagToFeatures\n","\n","    def trainModelAndLOOCVTest(self,akm,LagToData,LagToFeatures,target,objective,successfile):\n","        '''\n","        Args:\n","            akm -> akm string\n","            LagToData -> Dictionary with Lag as key and corresponding dataframe as value\n","            LagToFeatures -> Dictionary with Lag as key and corresponding Lag features as value\n","            target -> target feature name\n","            objective -> smape or r2\n","            successfile -> logfile\n","        Returns:\n","            It returns Best lag dataframe, Best lag value and Best lag features list\n","        '''\n","        #while strictly not necessary, assuming ordered dictionary for aesthetic reasons\n","        bestsmape = 1000000\n","        bestlag = 0\n","        bestsmapeskippedcnt = 0\n","        bestsmapedatalength = 0\n","        bestr2 = -1000000\n","        \n","        if (objective == 'smape'):\n","            for lag, df in LagToData.items():\n","                if (len(df.index) < 30):\n","                    print('Skipping modeling of lag = '+ str(lag) + 'for ' + akm + ' because numrows < 30')\n","                    continue\n","\n","                features = LagToFeatures[lag]\n","\n","                X = df[features].to_numpy()\n","                y = df[target].to_numpy()\n","\n","                loo = LeaveOneOut()\n","                act = []\n","                pred = []\n","                for train_index, test_index in loo.split(X):\n","                    X_train, X_test = X[train_index], X[test_index]\n","                    y_train, y_test = y[train_index], y[test_index]            \n","                    knn = KNeighborsRegressor(n_neighbors=5,weights=self.knnweight)\n","                    knn.fit(X_train,y_train)\n","                    y_test_pred = knn.predict(X_test)\n","                    #Assumption: LOOCV ==> only one element in the test data\n","                    #remember log(z+1) has been done on target\n","                    pred.append(math.exp(y_test_pred[0])-1)\n","                    act.append(math.exp(y_test[0])-1)\n","\n","                smapedatalength, skippedcnt, smape = self.computeSMAPENoExp(act,pred)\n","                r2 = round(r2_score(act,pred),2)\n","\n","                #Using SMAPE as the the criterion which must be minimized\n","                if (smape < bestsmape):\n","                    bestsmape = smape\n","                    bestlag = lag\n","                    bestsmapeskippedcnt = skippedcnt\n","                    bestsmapedatalength = smapedatalength\n","                    bestr2 = r2\n","\n","            bestlagdatalength = len(LagToData[bestlag].index)\n","            lag1datalength = len(LagToData[1].index)\n","            print('AKM\\ttarget\\tbestSMAPE\\tr2\\tskippedcnt\\tdatalength\\tlag = '+akm+'\\t'+target+'\\t'+str(bestsmape)+'\\t'+str(bestr2)+'\\t'+str(bestsmapeskippedcnt)+'\\t'+str(smapedatalength)+'\\t'+str(bestlag))\n","            successfile.write('AKM\\ttarget\\tbestSMAPE\\tr2\\tskippedcnt\\tdatalength\\tlag = '+akm+'\\t'+target+'\\t'+str(bestsmape)+'\\t'+str(bestr2)+'\\t'+str(bestsmapeskippedcnt)+'\\t'+str(smapedatalength)+'\\t'+str(bestlag)+'\\n')\n","            successfile.flush()\n","            return LagToData[bestlag], bestlag, LagToFeatures[bestlag]\n","        \n","        #objective is r2\n","        else:\n","            for lag, df in LagToData.items():\n","                if (len(df.index) < 30):\n","                    print('Skipping modeling of lag = '+ str(lag) + 'for ' + akm + ' because numrows < 30')\n","                    continue\n","\n","                features = LagToFeatures[lag]\n","\n","                X = df[features].to_numpy()\n","                y = df[target].to_numpy()\n","\n","                loo = LeaveOneOut()\n","                act = []\n","                pred = []\n","                for train_index, test_index in loo.split(X):\n","                    X_train, X_test = X[train_index], X[test_index]\n","                    y_train, y_test = y[train_index], y[test_index]            \n","                    knn = KNeighborsRegressor(n_neighbors=5,weights=self.knnweight)\n","                    knn.fit(X_train,y_train)\n","                    y_test_pred = knn.predict(X_test)\n","                    #Assumption: LOOCV ==> only one element in the test data\n","                    #remember log(z+1) has been done on target\n","                    pred.append(math.exp(y_test_pred[0])-1)\n","                    act.append(math.exp(y_test[0])-1)\n","\n","                smapedatalength, skippedcnt, smape = self.computeSMAPENoExp(act,pred)\n","                r2 = round(r2_score(act,pred),2)\n","\n","                #Using r2 as the the criterion which must be maximized\n","                if (r2 > bestr2):\n","                    bestsmape = smape\n","                    bestlag = lag\n","                    bestsmapeskippedcnt = skippedcnt\n","                    bestsmapedatalength = smapedatalength\n","                    bestr2 = r2\n","\n","            bestlagdatalength = len(LagToData[bestlag].index)\n","            lag1datalength = len(LagToData[1].index)\n","            \n","            print('AKM\\ttarget\\tSMAPE\\tbestr2\\tskippedcnt\\tdatalength\\tlag = '+akm+'\\t'+target+'\\t'+str(bestsmape)+'\\t'+str(bestr2)+'\\t'+str(bestsmapeskippedcnt)+'\\t'+str(smapedatalength)+'\\t'+str(bestlag))\n","            successfile.write('AKM\\ttarget\\tSMAPE\\tbestr2\\tskippedcnt\\tdatalength\\tlag = '+akm+'\\t'+target+'\\t'+str(bestsmape)+'\\t'+str(bestr2)+'\\t'+str(bestsmapeskippedcnt)+'\\t'+str(smapedatalength)+'\\t'+str(bestlag)+'\\n')\n","            successfile.flush()\n","            return LagToData[bestlag], bestlag, LagToFeatures[bestlag]\n","\n","    def pyplotscatter(self,akm,target,actual,predicted,setrange):\n","        '''\n","        Args:\n","            akm -> akm string \n","            target -> target feature name\n","            actual -> list/array ofactual values\n","            predicted -> list/array of predicted values\n","            setrange ->\n","        Returns:\n","            It will plot scattered plot of actual and predicted values with regression line\n","        '''\n","        x = actual\n","        y = predicted\n","        title = akm+'::'+target+'::test actual .vs predicted'+'::target actl min::'+str(round(min(x),2))+'::max::'+str(round(max(x),2))\n","        subtitle = akm+'::'+target+'::test actual .vs predicted'+'::target pred min::'+str(round(min(y),2))+'::max::'+str(round(max(y),2))\n","        plt.figure(figsize=(5,5))\n","        plt.title(title)\n","        plt.suptitle(subtitle)\n","        plt.scatter(x,y)\n","        plt.xlabel('actual')\n","        plt.ylabel('predicted')\n","        \n","        #calculate equation for trendline and add it to the plot\n","        try:\n","            z = np.polyfit(x, y, 1)\n","            p = np.poly1d(z)\n","            r_squared = r2_score(x, y)\n","            plt.plot(x, p(x), label=(\"$R^2$=%.6f\"%(r_squared)))\n","            plt.legend()\n","        except:\n","            pass\n","\n","        if (setrange):\n","            mini = min(min(x),min(y))\n","            maxi = max(max(x),max(y))\n","            plt.xlim(mini,maxi)\n","            plt.ylim(mini,maxi)\n","\n","        plt.show()\n","        time.sleep(0.05)\n","\n","    def get_stats(self,df,feature):\n","        '''\n","        Args :\n","            df -> input dataframe\n","            feature -> feature name for which statistics to be returned\n","        Returns :\n","            It returns min, max, mean, std and max_roc for the given feature from the training data\n","        '''\n","        max_val = round(df[feature].max(),2)   # get maximum value for the given feature\n","        mean_val = round(df[feature].mean(),2) # get mean value for the given feature\n","        std_val = round(df[feature].std(),2)   # get standard deviation for the given feature\n","        #max_roc = round(df[f'{feature}_DELTA(%)'].max(),2)  # get maximum roc from the delta values\n","\n","        non_zero_indicies = df[feature].to_numpy().nonzero()  # find non zero value indicies for given feature\n","        non_zero_values = df[feature].iloc[non_zero_indicies] # get the non zero value array\n","        min_val = min(non_zero_values)                        # get the minimum value from the non zero values\n"," \n","        return {'min':min_val,\n","                'max':max_val,\n","                'mean':mean_val,\n","                'std':std_val}\n","                #'max_roc':max_roc}\n","\n","    def serialize_model(self,model,root_path,AD_GROUP_ID,KEYWORD_ID,MATCH_TYPE):\n","        '''\n","        Args:\n","            model -> trained model object\n","            root_path -> path to store model\n","            AD_GROUP_ID -> value of AD_GROUP_ID\n","            KEYWORD_ID -> value of KEYWORD_ID\n","            MATCH_TYPE -> value of MATCH_TYPE\n","        Returns:\n","            It will serialize model for the given AKM combination and store it at root_path\n","        '''\n","        # year\n","        year = self.todays_date.strftime(\"%Y\")\n","        # Month\n","        month = self.todays_date.strftime(\"%m\")\n","        # Day\n","        day = self.todays_date.strftime(\"%d\")\n","        # join string with current year, month and day\n","        year_month_day =  str(year) + str(month) + str(day)\n","        \n","        # model name as a string of year_month_day_AD_GROUP_ID_KEYWORD_ID_MATCH_TYPE_{target_feature}Model\n","        model_name = '_'.join([year_month_day,\n","                               str(int(AD_GROUP_ID)),\n","                               str(int(KEYWORD_ID)),\n","                               str(MATCH_TYPE),\n","                               f'{self.target_feature}',\n","                               'KNNModel'])\n","        \n","        full_path = os.path.join(root_path,model_name) + '.pickle' # full model path with pickle format\n","        os.makedirs(root_path,exist_ok=True) # create directory if it does not exist\n","        joblib.dump(model,full_path) # dump model\n","        print(f'\\n ********** KNN model saved at {full_path} **********')\n","\n","    def serialize_akm_metadata(self,\n","                               df_akm,\n","                               model_root_path,\n","                               AD_GROUP_ID,\n","                               KEYWORD_ID,\n","                               MATCH_TYPE,\n","                               independent_features,\n","                               target_feature,\n","                               best_lag,\n","                               testsmape,\n","                               testdata_length,\n","                               skipped_smape_cnt,\n","                               date_range,\n","                               at_least_one_col_zero,\n","                               at_least_one_col_non_zero,\n","                               all_col_zero,\n","                               total_records,\n","                               total_records_best_lag_df,\n","                               best_lag_train_df_records,\n","                               isEnoughData_flag):\n","        '''\n","        Returns :\n","            It will serialize AKM values along with the stats of model as a json as binary format and store at a given location.\n","        '''\n","        \n","        # year\n","        year = self.todays_date.strftime(\"%Y\")\n","        # Month\n","        month = self.todays_date.strftime(\"%m\")\n","        # Day\n","        day = self.todays_date.strftime(\"%d\")\n","\n","        year_month_day =  str(year) + str(month) + str(day)\n","\n","        model_name = '_'.join([year_month_day, \n","                               str(int(AD_GROUP_ID)),\n","                               str(int(KEYWORD_ID)),\n","                               str(MATCH_TYPE),\n","                               f'{self.target_feature}',\n","                               'KNNModel'])\n","        \n","        model_stats = self.get_stats(df_akm,self.target_feature)\n","\n","        # create akm json to store at specified location\n","        akm_dict = {'AD_GROUP_ID' : int(AD_GROUP_ID),\n","                    'KEYWORD_ID' : int(KEYWORD_ID),\n","                    'MATCH_TYPE' : str(MATCH_TYPE),\n","                    'MODEL_VERSION' : str(self.todays_date),\n","                    'MODEL' : {\n","                        'NAME' : model_name,\n","                        'INDEPENDENT_FEATURES' : independent_features,\n","                        'TARGET_FEATURE' : target_feature,\n","                        'STATS' : {\n","                            'BEST_LAG' : best_lag,\n","                            'TEST_SMAPE' : testsmape,\n","                            'TEST_DATA_LENGTH' : testdata_length,\n","                            'SKIPPED_SMAPE_COUNT' : skipped_smape_cnt,\n","                            'DATE_RANGE' : date_range,\n","                            'AT_LEAST_ONE_COL_ZERO' : at_least_one_col_zero,\n","                            'AT_LEAST_ONE_COL_NON_ZERO' : at_least_one_col_non_zero,\n","                            'ALL_COL_ZERO' : all_col_zero,\n","                            'TOTAL_RECORDS' : total_records,\n","                            'TOTAL_RECORDS_BEST_LAG_DF' : total_records_best_lag_df,\n","                            'BEST_LAG_TRAIN_DF_RECORDS' : best_lag_train_df_records,\n","                            'IS_ENOUGH_DATA' : isEnoughData_flag,\n","                            f'{self.target_feature}_MIN' : model_stats['min'],\n","                            f'{self.target_feature}_MAX' : model_stats['max'],\n","                            f'{self.target_feature}_MEAN' : model_stats['mean'],\n","                            f'{self.target_feature}_SD' : model_stats['std']\n","                        }   \n","                    }\n","                }\n","                \n","        full_path = os.path.join(model_root_path,model_name)\n","        # dump json at specified location\n","        joblib.dump(akm_dict,filename=full_path+'.json')\n","        print('*'*10,'AKM metadata Dictionary saved at {}'.format(full_path+'.json'),'*'*10,'\\n')\n","        return 'AKM metadata Dictionary saved at {}'.format(model_root_path)\n","\n","    def finalKNNModelTrainAndTest(self,akm,bestLag,bestLagData,bestFeatures,target,testsize,successfile):\n","        '''\n","        Args:\n","            akm -> akm string\n","            bestLag -> bestLag value\n","            bestLagData -> bestLag dataframe\n","            bestFeatures -> bestLag features\n","            target -> target feature name\n","            testsize -> test size (0.20)\n","            successfile -> logfile\n","        Returns:\n","            it will train KNN model on 80% data and test on 20% data and returns dictionary of \n","           'bestLag' ,'smapedatalength', 'skippedcnt', 'testsmape'\n","        '''\n","        X = bestLagData[bestFeatures].to_numpy()\n","        y = bestLagData[target].to_numpy()\n","\n","        # train test split \n","        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=testsize,random_state=0)\n","        knn = KNeighborsRegressor(n_neighbors=5,weights=self.knnweight)\n","        knn.fit(X_train,y_train)\n","        y_test_pred = knn.predict(X_test)\n","\n","        # remember log(z+1) xfrm has been used on target\n","        ytestreg = np.exp(y_test)-1\n","        ypredreg = np.exp(y_test_pred)-1\n","\n","        smapedatalength, skippedcnt, testsmape = self.computeSMAPENoExp(ytestreg.tolist(),ypredreg.tolist())\n","        testr2 = round(r2_score(ytestreg, ypredreg),2)\n","        print('AKM\\ttarget\\ttestSMAPE\\tr2\\tskippedcnt\\tdatalength\\tLag = '+akm+'\\t'+target+'\\t'+str(testsmape)+'\\t'+str(testr2)+'\\t'+str(skippedcnt)+'\\t'+str(smapedatalength)+'\\t'+str(bestLag))\n","        successfile.write('AKM\\ttarget\\ttestSMAPE\\tr2\\tskippedcnt\\tdatalength\\tLag = '+akm+'\\t'+target+'\\t'+str(testsmape)+'\\t'+str(testr2)+'\\t'+str(skippedcnt)+'\\t'+str(smapedatalength)+'\\t'+str(bestLag)+'\\n')\n","        successfile.flush()\n","        self.pyplotscatter(akm,target,ytestreg,ypredreg,True)\n","        return {'bestLag' : bestLag, 'smapedatalength' : smapedatalength, 'skippedcnt' : skippedcnt, 'testsmape' : testsmape, 'testr2' : testr2}\n","\n","    def finalKNNModel(self,bestLagData,bestFeatures,target):\n","        '''\n","        Args:\n","            bestLagData -> bestLag dataframe\n","            bestFeatures -> bestLag feature list\n","            target -> target feature name\n","        Returns:\n","            It will train KNN model on 100% data(bestLag Data)\n","        '''\n","        X = bestLagData[bestFeatures].to_numpy()\n","        y = bestLagData[target].to_numpy()\n","        knn = KNeighborsRegressor(n_neighbors=5,weights=self.knnweight)\n","        knn.fit(X,y)\n","        return knn\n","\n","    def trainModels(self,input_path,akm_to_model_path,logfile_path,model_path):\n","        '''\n","        Args:\n","            input_path -> input path with all akms data \n","            akm_to_model_path -> Csv of the AD_GROUP_ID, KEYWORD_ID and MATCH_TYPE to train KNN models only\n","            logfile_path -> Path to store logfile\n","            model_path -> Path to store trained models\n","        Returns:\n","            It will train and store models/jsons for each akm and return statistics dataframe\n","        '''\n","        fulldf = self.readcsv(input_path) # fulldf means selected akm joined file with original file on 19th october\n","        akms2model = pd.read_csv(akm_to_model_path)\n","\n","        # akms2model = fulldf.copy()\n","        # akms2model = akms2model.groupby(['AD_GROUP_ID','KEYWORD_ID','MATCH_TYPE']).size().reset_index().rename(columns={0:'count'})\n","        \n","        print('Number of AKMs to model = '+ str(akms2model.shape[0]))\n","        count = 0\n","        \n","        os.makedirs(logfile_path,exist_ok=True) # create directory if it does not exist\n","        logfile_full_path = os.path.join(logfile_path,'trainsuccess.log') # full log path\n","        successfile = open(logfile_full_path,'w')\n","    \n","        akm_list = []\n","        bestLag_list = []\n","        smape_test_datalength_list = []\n","        skippedcnt_list = []\n","        testsmape_list = []\n","        testr2_list = []\n","        date_range_list = []\n","        at_least_one_col_zero_list = []\n","        at_least_one_col_non_zero_list = []\n","        all_col_zero_list = []\n","        total_records_list = []\n","        total_records_best_lag_df_list = []\n","        best_lag_train_df_records_list = []\n","        isEnoughData_flag_list = []\n","\n","        for index, row in akms2model.iterrows():\n","            print(f'\\n ************************** Iteration {index+1} ************************************ \\n')\n","            AD_GROUP_ID = row['AD_GROUP_ID']\n","            KEYWORD_ID = row['KEYWORD_ID']\n","            MATCH_TYPE = row['MATCH_TYPE']\n","            # akm = str(AD_GROUP_ID)+'-'+str(KEYWORD_ID)+'-'+MATCH_TYPE\n","            # testakm = '157188419817864-177620140515274-PHRASE'\n","            # if (akm != testakm):\n","            #     continue\n","            df = self.getakmdata(fulldf,AD_GROUP_ID,KEYWORD_ID,MATCH_TYPE)\n","            df.to_csv('./akmtrainingdata.csv')\n","            # if 'ROAS_TARGET' in self.independent_features:\n","            #     df['ROAS_TARGET'] = np.log(((df['SALES_USD']+1)/(df['SPEND_USD']+1)))\n","            df_temp = df.copy()\n","            if ('ROAS_TARGET' in self.independent_features):\n","                df_temp['ROAS_TARGET'] = (df_temp['SALES_USD']+1)/(df_temp['SPEND_USD']+1)\n","            \n","            # create list of all features\n","            features_to_check_data = self.independent_features + [self.target_feature]\n","            # calculate date_range\n","            date_range = (df_temp.REPORT_DATE.max() - df_temp.REPORT_DATE.min()).days + 1                    # to get the number of days\n","            at_least_one_col_zero = len(df_temp.loc[(df_temp[features_to_check_data] == 0).any(axis=1)])     # count of at least on column zero\n","            at_least_one_col_non_zero = len(df_temp.loc[(df_temp[features_to_check_data] != 0).any(axis=1)]) # count of at least on column non zero\n","            all_col_zero = len(df_temp.loc[(df_temp[features_to_check_data] == 0).all(axis=1)])              # count of all column zero\n","            total_records = len(df_temp)                                                                # total records in df\n","\n","            statcols = [self.target_feature]\n","            if (self.isEnoughData(df,statcols)):\n","                pass\n","            else:\n","                print('AKM = ' + akm + ' does not have enough data and is skipped')\n","                count += 1\n","                if (count%100 == 0):\n","                    print(\"done with \" + str(count) + \" out of \" + str(akms2model.shape[0]))\n","                continue\n","            \n","            for feature in features_to_check_data:\n","                if feature == 'ROAS_TARGET':\n","                    df['LNROAS_TARGET'] = np.log(((df['SALES_USD']+1)/(df['SPEND_USD']+1)))\n","    \n","                else:\n","                    df[f'LN{feature}'] = np.log(df[feature]+1)\n","            \n","            try:\n","                features = [f'LN{feature}' for feature in self.independent_features]\n","                target = f'LN{self.target_feature}'\n","                maxlag = self.max_lag\n","                LagToData,LagToFeatures = self.buildLaggedData(akm,df,features,target,maxlag)\n","                objective = 'smape'\n","                bestLagData,bestLag,bestLagFeatures = self.trainModelAndLOOCVTest(akm,LagToData,LagToFeatures,target,objective,successfile)\n","\n","                #train final model and test\n","                testsize = 0.2\n","                knn_output = self.finalKNNModelTrainAndTest(akm,bestLag,bestLagData,bestLagFeatures,target,testsize,successfile)\n","\n","                smape_datalength = knn_output['smapedatalength']\n","                skipped_cnt = knn_output['skippedcnt']\n","                testsmape = knn_output['testsmape']\n","                testr2 = knn_output['testr2']\n","                total_records_best_lag_df = len(bestLagData)\n","                best_lag_train_df_records = int(len(bestLagData)*0.80)\n","                isEnoughData_flag = self.isEnoughData(df,statcols)\n","                \n","                # append akm wise statistics for comparision purpose\n","                akm_list.append(akm)\n","                bestLag_list.append(bestLag)\n","                smape_test_datalength_list.append(smape_datalength)\n","                skippedcnt_list.append(skipped_cnt)\n","                testsmape_list.append(testsmape)\n","                testr2_list.append(testr2)\n","                date_range_list.append(date_range)\n","                at_least_one_col_zero_list.append(at_least_one_col_zero)\n","                at_least_one_col_non_zero_list.append(at_least_one_col_non_zero)\n","                all_col_zero_list.append(all_col_zero)\n","                total_records_list.append(total_records)\n","                total_records_best_lag_df_list.append(total_records_best_lag_df)\n","                best_lag_train_df_records_list.append(best_lag_train_df_records)\n","                isEnoughData_flag_list.append(isEnoughData_flag)\n","\n","                # train final KNN model and seriaize in .pickle format\n","                trained_knn_model = self.finalKNNModel(bestLagData=bestLagData,bestFeatures=bestLagFeatures,target=target)\n","\n","                # serialize final knn model\n","                self.serialize_model(model=trained_knn_model,\n","                                     root_path=model_path,\n","                                     AD_GROUP_ID=AD_GROUP_ID,\n","                                     KEYWORD_ID=KEYWORD_ID,\n","                                     MATCH_TYPE=MATCH_TYPE)\n","                \n","                # serialize metadata json\n","                self.serialize_akm_metadata(df_akm=df,\n","                                            model_root_path=model_path,\n","                                            AD_GROUP_ID=AD_GROUP_ID,\n","                                            KEYWORD_ID=KEYWORD_ID,\n","                                            MATCH_TYPE=MATCH_TYPE,\n","                                            independent_features=self.independent_features,\n","                                            target_feature=self.target_feature,\n","                                            best_lag=bestLag,\n","                                            testsmape=testsmape,\n","                                            testdata_length=smape_datalength,\n","                                            skipped_smape_cnt=skipped_cnt,\n","                                            date_range=date_range, \n","                                            at_least_one_col_zero=at_least_one_col_zero,\n","                                            at_least_one_col_non_zero=at_least_one_col_non_zero,\n","                                            all_col_zero=all_col_zero,\n","                                            total_records=total_records,\n","                                            total_records_best_lag_df=total_records_best_lag_df,\n","                                            best_lag_train_df_records=best_lag_train_df_records, \n","                                            isEnoughData_flag=isEnoughData_flag)\n","                \n","                \n","            except:\n","                print('AKM = '+akm+' training failed ' + str(traceback.format_exc())+'\\n')\n","\n","            count += 1\n","            if (count%100 == 0):\n","                print(\"done with \" + str(count) + \" out of \" + str(akms2model.shape[0]))\n","                print(datetime.now())\n","        \n","        # dataframe of akm statistics for comparision purpose\n","        df_all_metrics = pd.DataFrame({\n","            'akm':akm_list,\n","            'date_range':date_range_list,\n","            'total_records':total_records_list,\n","            'isEnoughData_flag' : isEnoughData_flag_list,\n","            'at_least_one_col_zero':at_least_one_col_zero_list,\n","            'at_least_one_col_non_zero':at_least_one_col_non_zero_list,\n","            'all_col_zero':all_col_zero_list,\n","            'bestLag':bestLag_list,\n","            'best_lag_df_total_records':total_records_best_lag_df_list,\n","            'best_lag_df_train_records' : best_lag_train_df_records_list,\n","            'smape_test_datalength':smape_test_datalength_list,\n","            'smape_skippedcnt':skippedcnt_list,\n","            'testsmape':testsmape_list,\n","            'testr2':testr2_list\n","        })                   \n","        successfile.close()\n","        return df_all_metrics\n","\n","    \n","    def main(self):\n","        df_all_metrics = self.trainModels(input_path=self.input_path,\n","                                          akm_to_model_path=self.akm_to_model_path,\n","                                          logfile_path=self.logfile_path,\n","                                          model_path=self.model_path)\n","\n","        return df_all_metrics"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9h216V7NFddU","executionInfo":{"status":"ok","timestamp":1668896369727,"user_tz":480,"elapsed":160,"user":{"displayName":"Sukrit Ganesh","userId":"09882607507987906814"}}},"outputs":[],"source":["input_path='akmtrainingdata.csv'\n","akm_to_model_path = 'akm.csv'\n","model_path='KNNModelsDistanceWeighing'\n","logfile_path=model_path\n","max_lag = 15\n","knnweight = 'distance'\n","independent_features=['CPC_USD','SALES_USD','CONVERSIONS','CLICKS','IMPRESSIONS','SPEND_USD','ROAS_TARGET']\n","target_feature = 'CPC_USD'"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_tyPZixypLEN","executionInfo":{"status":"ok","timestamp":1668896373954,"user_tz":480,"elapsed":170,"user":{"displayName":"Sukrit Ganesh","userId":"09882607507987906814"}}},"outputs":[],"source":["knn_modeling = KnnModelTrain(input_path=input_path,\n","                             akm_to_model_path = akm_to_model_path,\n","                             logfile_path=logfile_path,\n","                             model_path=model_path,\n","                             independent_features=independent_features,\n","                             target_feature = target_feature,\n","                             max_lag=max_lag,\n","                             knnweight=knnweight)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"elapsed":620,"status":"error","timestamp":1668896376926,"user":{"displayName":"Sukrit Ganesh","userId":"09882607507987906814"},"user_tz":480},"id":"EJSxLB1hmnmd","outputId":"44302b06-49c4-4383-ad83-bb84a8bea1f9"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-32029b32e52a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_all_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_modeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-be7316607713>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m                                           \u001b[0makm_to_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0makm_to_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                                           \u001b[0mlogfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                                           model_path=self.model_path)\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf_all_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-be7316607713>\u001b[0m in \u001b[0;36mtrainModels\u001b[0;34m(self, input_path, akm_to_model_path, logfile_path, model_path)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mIt\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstore\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mjsons\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0makm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         '''\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fulldf means selected akm joined file with original file on 19th october\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0makms2model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0makm_to_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-be7316607713>\u001b[0m in \u001b[0;36mreadcsv\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mIt\u001b[0m \u001b[0mreads\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msorted\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mby\u001b[0m \u001b[0mAKM\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mREPORT_DATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         '''\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mfullakmdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'REPORT_DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mfullakmdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AD_GROUP_ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'KEYWORD_ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MATCH_TYPE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'REPORT_DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfullakmdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'akmtrainingdata.csv'"]}],"source":["df_all_metrics = knn_modeling.main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZT8xFxIFwYnR"},"outputs":[],"source":["df_all_metrics"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}